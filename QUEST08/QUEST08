{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dellaanima/AIFFEL_QUEST/blob/master/QUEST08/QUEST08\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4eba642",
      "metadata": {
        "id": "e4eba642"
      },
      "source": [
        "# 16. 프로젝트: 한국어 데이터로 챗봇 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2175e8fc",
      "metadata": {
        "id": "2175e8fc"
      },
      "source": [
        "## 목차\n",
        "\n",
        "1. 데이터 수집하기\n",
        "2. 데이터 전처리하기\n",
        "3. SubwordTextEncoder 사용하기\n",
        "4. 모델 구성하기\n",
        "5. 모델 평가하기\n",
        "6. 회고"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc278ad0",
      "metadata": {
        "id": "dc278ad0"
      },
      "source": [
        "## Step 1. 데이터 수집하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab3f7bb5",
      "metadata": {
        "id": "ab3f7bb5"
      },
      "source": [
        "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.  \n",
        "[songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7439a0ea",
      "metadata": {
        "id": "7439a0ea"
      },
      "outputs": [],
      "source": [
        "#필요한 패키지 import\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17762a5b",
      "metadata": {
        "id": "17762a5b"
      },
      "outputs": [],
      "source": [
        "#데이터 확인\n",
        "data_path = '/aiffel/data/ChatbotData .csv'\n",
        "data = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d74959e9",
      "metadata": {
        "id": "d74959e9",
        "outputId": "78875c91-0030-4a1e-8d24-22f591e31b82"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ]
          },
          "execution_count": 241,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f7b9aa",
      "metadata": {
        "id": "e7f7b9aa",
        "outputId": "fb0bcdd4-3da7-49e4-c996-eb3c70faa8a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11823"
            ]
          },
          "execution_count": 242,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a6488a",
      "metadata": {
        "id": "79a6488a"
      },
      "outputs": [],
      "source": [
        "max_sample=11823"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17506071",
      "metadata": {
        "id": "17506071"
      },
      "source": [
        "## Step 2. 데이터 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac890f3",
      "metadata": {
        "id": "5ac890f3",
        "outputId": "43fe2986-65b1-4570-9d5a-fcb2e3423b45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Q        0\n",
              "A        0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#null값 있는지 확인\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053a5444",
      "metadata": {
        "id": "053a5444",
        "outputId": "d631e2ce-3aae-4438-cd25-a42f848f3acb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 245,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#중복데이터 확인\n",
        "data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fac34af",
      "metadata": {
        "id": "5fac34af"
      },
      "source": [
        "아래의 함수는 정규 표현식(Regular Expression) 을 사용하여 구두점(punctuation) 을 제거하여 단어를 토크나이징(tokenizing) 하는 일에 방해가 되지 않도록 정제하는 것을 목표로 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4454e19d",
      "metadata": {
        "id": "4454e19d",
        "outputId": "28cc9ffd-7d58-45a3-bd32-8fdf1d33564d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "    # 입력받은 sentence를 양쪽 공백 제거\n",
        "    sentence = sentence.strip() # 공백 제거\n",
        "\n",
        "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "    # (가-힣,a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
        "    pattern2 = \"0-9가-힣.?!,\"\n",
        "    pattern1 = \"가-힣a-zA-Z.?!,\"\n",
        "    pattern = \"가-힣.?!,\"\n",
        "    sentence = re.sub(pattern1, ' ', sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00013ef6",
      "metadata": {
        "id": "00013ef6",
        "outputId": "5d52d223-6672-4986-9531-11346307f0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
        "def load_conversations():\n",
        "    inputs, outputs = [], []\n",
        "    for q in data['Q']:\n",
        "        inputs.append(preprocess_sentence(q))\n",
        "    for a in data['A']:\n",
        "        outputs.append(preprocess_sentence(a))\n",
        "    if len(inputs) >= max_sample:\n",
        "        return inputs, outputs\n",
        "    return inputs, outputs\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3545609a",
      "metadata": {
        "id": "3545609a",
        "outputId": "e1e7e8f2-8e89-4df9-9a02-0af6a926982b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전체 샘플 수 : 11823\n",
            "전체 샘플 수 : 11823\n"
          ]
        }
      ],
      "source": [
        "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
        "questions, answers = load_conversations()\n",
        "print('전체 샘플 수 :', len(questions))\n",
        "print('전체 샘플 수 :', len(answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e968a86b",
      "metadata": {
        "id": "e968a86b",
        "outputId": "07b5c6fd-4603-4173-db18-a5714c6687b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전처리 후의 222번째 질문 샘플: 공부 왜 해야 돼 ?\n",
            "전처리 후의 222번째 답변 샘플: 공부하면 더 많은 선택을 할 수 있죠 .\n"
          ]
        }
      ],
      "source": [
        "print('전처리 후의 222번째 질문 샘플: {}'.format(questions[221]))\n",
        "print('전처리 후의 222번째 답변 샘플: {}'.format(answers[221]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687b0a4a",
      "metadata": {
        "id": "687b0a4a"
      },
      "source": [
        "## Step 3. SubwordTextEncoder 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "627428e0",
      "metadata": {
        "id": "627428e0"
      },
      "source": [
        "### 단어장 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddbb190e",
      "metadata": {
        "id": "ddbb190e"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a141b3",
      "metadata": {
        "id": "48a141b3",
        "outputId": "8103fe12-647f-4d05-adfe-d7e26340bda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20fafdad",
      "metadata": {
        "id": "20fafdad"
      },
      "source": [
        "시작 토큰과 종료 토큰에 부여된 정수를 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d937607f",
      "metadata": {
        "id": "d937607f",
        "outputId": "37b63d14-810b-43da-eb1d-d0768d463e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "START_TOKEN의 번호 : [8173]\n",
            "END_TOKEN의 번호 : [8174]\n"
          ]
        }
      ],
      "source": [
        "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
        "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f1fa71b",
      "metadata": {
        "id": "7f1fa71b",
        "outputId": "490bd187-4c64-4f41-c72a-21681e32dc6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8175\n"
          ]
        }
      ],
      "source": [
        "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "print(VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de091261",
      "metadata": {
        "id": "de091261"
      },
      "source": [
        "### 2. 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffce1de2",
      "metadata": {
        "id": "ffce1de2",
        "outputId": "61a9fa1c-963c-4b76-dd7a-d5a1844c760b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 222번째 질문 샘플: [549, 66, 217, 142, 2]\n",
            "정수 인코딩 후의 222번째 답변 샘플: [7767, 7, 162, 1862, 35, 4, 299, 1]\n"
          ]
        }
      ],
      "source": [
        "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
        "# 각 토큰을 고유한 정수로 변환\n",
        "print('정수 인코딩 후의 222번째 질문 샘플: {}'.format(tokenizer.encode(questions[221])))\n",
        "print('정수 인코딩 후의 222번째 답변 샘플: {}'.format(tokenizer.encode(answers[221])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18287716",
      "metadata": {
        "id": "18287716",
        "outputId": "f1157e31-63f1-4a7f-aa9b-fcd010330355"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_sentence_lengths(sentences):\n",
        "    # 각 문장의 단어 수를 세어줍니다\n",
        "    word_counts = [len(sentence.split()) for sentence in sentences]\n",
        "\n",
        "    # 최대 단어 수를 계산하여 히스토그램의 구간 수를 설정합니다\n",
        "    max_word_count = max(word_counts)\n",
        "\n",
        "    # 단어 수에 대한 히스토그램을 생성합니다\n",
        "    # align='left': 이 매개변수는 히스토그램의 막대 정렬을 제어\n",
        "    # rwidth=0.8: 이 매개변수는 히스토그램에서 막대의 상대적 너비를 제어\n",
        "    plt.hist(word_counts, bins=range(1, max_word_count + 2), align='left', rwidth=0.8)\n",
        "    plt.xlabel('words')\n",
        "    plt.ylabel('sentence')\n",
        "    plt.xticks(range(1, max_word_count + 1))\n",
        "    plt.show()\n",
        "\n",
        "visualize_sentence_lengths(questions)\n",
        "visualize_sentence_lengths(answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d171e2bf",
      "metadata": {
        "scrolled": true,
        "id": "d171e2bf",
        "outputId": "790cd576-be0b-42cb-a60f-2f60859edfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        }
      ],
      "source": [
        "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
        "MAX_LENGTH = 16\n",
        "print(MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bdc801",
      "metadata": {
        "id": "c7bdc801",
        "outputId": "6c5b0669-3c13-488a-a4aa-68ba432e1743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    # 최대 길이 40으로 모든 데이터셋을 패딩\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    tokenized_inputs, maxlen=MAX_LENGTH, padding='pre')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    tokenized_outputs, maxlen=MAX_LENGTH, padding='pre')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b7600e",
      "metadata": {
        "scrolled": true,
        "id": "a6b7600e",
        "outputId": "652afc3f-e42f-4759-ed33-99565658f1fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어장의 크기 : 8175\n",
            "필터링 후의 질문 샘플 개수: 11673\n",
            "필터링 후의 답변 샘플 개수: 11673\n"
          ]
        }
      ],
      "source": [
        "questions, answers = tokenize_and_filter(questions, answers)\n",
        "print('단어장의 크기 :',(VOCAB_SIZE))\n",
        "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
        "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c26aaa",
      "metadata": {
        "id": "18c26aaa"
      },
      "source": [
        "### 교사 강요 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7eb8aa5",
      "metadata": {
        "id": "b7eb8aa5",
        "outputId": "c506237f-7b3a-4aea-ddf6-4ceb5f9a6b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0958c01",
      "metadata": {
        "id": "d0958c01"
      },
      "source": [
        "## Step 4. 모델 구성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df890c1",
      "metadata": {
        "id": "9df890c1"
      },
      "source": [
        "포지셔널 인코딩 레이어 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd18e0b3",
      "metadata": {
        "id": "fd18e0b3",
        "outputId": "3b96c678-f2e1-46be-8efb-19222a16a76b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "#포지셔널 행렬을 직접 구현\n",
        "# 포지셔널 인코딩 레이어\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        # 각도 배열 생성\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model)\n",
        "\n",
        "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        # sin과 cosine이 교차되도록 재배열\n",
        "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
        "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0])\n",
        "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
        "\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f89ea2",
      "metadata": {
        "id": "e1f89ea2"
      },
      "source": [
        " 스케일드 닷 프로덕트 어텐션 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d746a15",
      "metadata": {
        "id": "2d746a15",
        "outputId": "14996343-a654-43c9-ad54-e6e4ac6df54d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 스케일드 닷 프로덕트 어텐션 함수\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # 가중치를 정규화\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # 패딩에 마스크 추가\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "        # softmax적용\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "    return output\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e65d7a4",
      "metadata": {
        "id": "7e65d7a4"
      },
      "source": [
        "MultiHeadAttention 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa8bf43",
      "metadata": {
        "id": "cfa8bf43",
        "outputId": "be016281-4e62-4dc7-f7c2-48f378c27f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(\n",
        "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # Q, K, V에 각각 Dense를 적용합니다\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # 스케일드 닷 프로덕트 어텐션 함수\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85aaf9e2",
      "metadata": {
        "id": "85aaf9e2"
      },
      "source": [
        "패딩 마스킹 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2732d1e",
      "metadata": {
        "id": "b2732d1e",
        "outputId": "3e6325a1-fe11-4994-b476-460691e29eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "#패딩 마스킹을 구현한 함수\n",
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6c73a6",
      "metadata": {
        "id": "da6c73a6",
        "outputId": "6bb7447a-76b5-40bb-b863-cdf7d65ce96a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65394e46",
      "metadata": {
        "id": "65394e46"
      },
      "source": [
        "인코더 만들기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f89fd1e",
      "metadata": {
        "id": "0f89fd1e",
        "outputId": "eee2498c-afa6-4814-80e9-8d6107a1545e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 인코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "    # 패딩 마스크 사용\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "    attention = MultiHeadAttention(\n",
        "        d_model, num_heads, name=\"attention\")({\n",
        "        'query': inputs,\n",
        "        'key': inputs,\n",
        "        'value': inputs,\n",
        "        'mask': padding_mask\n",
        "    })\n",
        "\n",
        "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a751ba",
      "metadata": {
        "id": "f0a751ba",
        "outputId": "ba999bbb-9402-4afe-836d-2504c0f7cc30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "    # 패딩 마스크 사용\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    # 임베딩 레이어\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "    # 포지셔널 인코딩\n",
        "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    # num_layers만큼 쌓아올린 인코더의 층.\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51e951ce",
      "metadata": {
        "id": "51e951ce"
      },
      "source": [
        "디코더 만들기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac2b196",
      "metadata": {
        "id": "0ac2b196",
        "outputId": "b8ce5b80-eb2e-468d-c189-e260a0ff3b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 디코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "    attention1 = MultiHeadAttention(\n",
        "    d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "        'query': inputs,\n",
        "        'key': inputs,\n",
        "        'value': inputs,\n",
        "        'mask': look_ahead_mask\n",
        "    })\n",
        "\n",
        "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "    attention1 = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
        "    attention2 = MultiHeadAttention(\n",
        "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "        'query': attention1,\n",
        "        'key': enc_outputs,\n",
        "        'value': enc_outputs,\n",
        "        'mask': padding_mask\n",
        "    })\n",
        "\n",
        "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
        "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "        outputs=outputs,\n",
        "        name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b928985",
      "metadata": {
        "id": "7b928985",
        "outputId": "c1db1ff3-be76-4fd2-e82d-8c8f13c1b858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "        shape=(1, None, None), name='look_ahead_mask')\n",
        "\n",
        "    # 패딩 마스크\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    # 임베딩 레이어\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "    # 포지셔널 인코딩\n",
        "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "        outputs=outputs,\n",
        "        name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e92e584",
      "metadata": {
        "id": "2e92e584"
      },
      "source": [
        "transformer 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e3b6f91",
      "metadata": {
        "id": "8e3b6f91",
        "outputId": "e1002c3c-2652-4a81-9346-cf81687f4171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    # 인코더에서 패딩을 위한 마스크\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "        create_padding_mask, output_shape=(1, 1, None),\n",
        "        name='enc_padding_mask')(inputs)\n",
        "\n",
        "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
        "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "        create_look_ahead_mask,\n",
        "        output_shape=(1, None, None),\n",
        "        name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "    # 디코더에서 패딩을 위한 마스크\n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "        create_padding_mask, output_shape=(1, 1, None),\n",
        "        name='dec_padding_mask')(inputs)\n",
        "\n",
        "    # 인코더\n",
        "    enc_outputs = encoder(\n",
        "        vocab_size=vocab_size,\n",
        "        num_layers=num_layers,\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "    )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    # 디코더\n",
        "    dec_outputs = decoder(\n",
        "        vocab_size=vocab_size,\n",
        "        num_layers=num_layers,\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    # 완전연결층\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6a4145",
      "metadata": {
        "id": "0e6a4145"
      },
      "source": [
        "### 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87389790",
      "metadata": {
        "id": "87389790",
        "outputId": "4ee6ee53-aff0-4b7b-b3fd-d010829d4216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inputs (InputLayer)             [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Functional)            (None, None, 256)    3147008     inputs[0][0]                     \n",
            "                                                                 enc_padding_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Functional)            (None, None, 256)    3674368     dec_inputs[0][0]                 \n",
            "                                                                 encoder[0][0]                    \n",
            "                                                                 look_ahead_mask[0][0]            \n",
            "                                                                 dec_padding_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "outputs (Dense)                 (None, None, 8175)   2100975     decoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 8,922,351\n",
            "Trainable params: 8,922,351\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 하이퍼파라미터\n",
        "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
        "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
        "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수\n",
        "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
        "DROPOUT = 0.4 # 드롭아웃의 비율\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd712531",
      "metadata": {
        "id": "fd712531"
      },
      "source": [
        "손실 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a2fef9",
      "metadata": {
        "id": "10a2fef9",
        "outputId": "d64dbf4e-701f-4916-fd0c-135916bd85a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b96319cb",
      "metadata": {
        "id": "b96319cb"
      },
      "source": [
        "3. 커스텀 된 학습률(Learning rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa1d0e2b",
      "metadata": {
        "id": "aa1d0e2b",
        "outputId": "88cf0ec9-a3e7-4ba7-8889-83fb5a902596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47fb6f3",
      "metadata": {
        "id": "d47fb6f3",
        "outputId": "b758ac4d-a69e-463f-ab70-f0e7deceac26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "execution_count": 228,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "337c72c0",
      "metadata": {
        "id": "337c72c0"
      },
      "source": [
        "모델 컴파일"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f74d34c",
      "metadata": {
        "id": "5f74d34c",
        "outputId": "2e96363e-5f9c-43f8-c734-78169d3262d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6dee5ff",
      "metadata": {
        "id": "c6dee5ff",
        "outputId": "bc0298e9-a33b-46c8-f191-5d9167350189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "183/183 [==============================] - 12s 35ms/step - loss: 4.3030 - accuracy: 0.0729\n",
            "Epoch 2/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 3.3677 - accuracy: 0.1820\n",
            "Epoch 3/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 2.6625 - accuracy: 0.1967\n",
            "Epoch 4/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 2.4130 - accuracy: 0.2041\n",
            "Epoch 5/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 2.2884 - accuracy: 0.2089\n",
            "Epoch 6/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 2.2018 - accuracy: 0.2130\n",
            "Epoch 7/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 2.1275 - accuracy: 0.2177\n",
            "Epoch 8/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 2.0481 - accuracy: 0.2234\n",
            "Epoch 9/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 1.9579 - accuracy: 0.2287\n",
            "Epoch 10/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 1.8573 - accuracy: 0.2365\n",
            "Epoch 11/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 1.7445 - accuracy: 0.2467\n",
            "Epoch 12/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 1.6233 - accuracy: 0.2588\n",
            "Epoch 13/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 1.4925 - accuracy: 0.2734\n",
            "Epoch 14/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 1.3607 - accuracy: 0.2887\n",
            "Epoch 15/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 1.2370 - accuracy: 0.3039\n",
            "Epoch 16/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 1.1162 - accuracy: 0.3190\n",
            "Epoch 17/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 1.0092 - accuracy: 0.3326\n",
            "Epoch 18/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.9108 - accuracy: 0.3456\n",
            "Epoch 19/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.8260 - accuracy: 0.3575\n",
            "Epoch 20/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.7492 - accuracy: 0.3679\n",
            "Epoch 21/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.6838 - accuracy: 0.3775\n",
            "Epoch 22/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.6263 - accuracy: 0.3856\n",
            "Epoch 23/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.5716 - accuracy: 0.3951\n",
            "Epoch 24/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.5177 - accuracy: 0.4035\n",
            "Epoch 25/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.4695 - accuracy: 0.4114\n",
            "Epoch 26/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.4314 - accuracy: 0.4182\n",
            "Epoch 27/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.3991 - accuracy: 0.4240\n",
            "Epoch 28/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.3679 - accuracy: 0.4296\n",
            "Epoch 29/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.3392 - accuracy: 0.4357\n",
            "Epoch 30/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.3134 - accuracy: 0.4395\n",
            "Epoch 31/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.2915 - accuracy: 0.4441\n",
            "Epoch 32/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.2708 - accuracy: 0.4486\n",
            "Epoch 33/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.2555 - accuracy: 0.4517\n",
            "Epoch 34/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.2383 - accuracy: 0.4555\n",
            "Epoch 35/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.2231 - accuracy: 0.4589\n",
            "Epoch 36/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.2078 - accuracy: 0.4616\n",
            "Epoch 37/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1964 - accuracy: 0.4644\n",
            "Epoch 38/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1874 - accuracy: 0.4663\n",
            "Epoch 39/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1766 - accuracy: 0.4681\n",
            "Epoch 40/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1654 - accuracy: 0.4715\n",
            "Epoch 41/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1592 - accuracy: 0.4728\n",
            "Epoch 42/100\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.1523 - accuracy: 0.4741\n",
            "Epoch 43/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1447 - accuracy: 0.4769\n",
            "Epoch 44/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1376 - accuracy: 0.4780\n",
            "Epoch 45/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1319 - accuracy: 0.4794\n",
            "Epoch 46/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1270 - accuracy: 0.4810\n",
            "Epoch 47/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1221 - accuracy: 0.4815\n",
            "Epoch 48/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1178 - accuracy: 0.4832\n",
            "Epoch 49/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1118 - accuracy: 0.4846\n",
            "Epoch 50/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1083 - accuracy: 0.4856\n",
            "Epoch 51/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1039 - accuracy: 0.4868\n",
            "Epoch 52/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.1006 - accuracy: 0.4871\n",
            "Epoch 53/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0961 - accuracy: 0.4886\n",
            "Epoch 54/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0965 - accuracy: 0.4887\n",
            "Epoch 55/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0912 - accuracy: 0.4901\n",
            "Epoch 56/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0893 - accuracy: 0.4906\n",
            "Epoch 57/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0861 - accuracy: 0.4918\n",
            "Epoch 58/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0825 - accuracy: 0.4924\n",
            "Epoch 59/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0815 - accuracy: 0.4922\n",
            "Epoch 60/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0784 - accuracy: 0.4930\n",
            "Epoch 61/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0768 - accuracy: 0.4939\n",
            "Epoch 62/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0747 - accuracy: 0.4942\n",
            "Epoch 63/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0744 - accuracy: 0.4942\n",
            "Epoch 64/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0716 - accuracy: 0.4950\n",
            "Epoch 65/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0694 - accuracy: 0.4956\n",
            "Epoch 66/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0689 - accuracy: 0.4959\n",
            "Epoch 67/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0658 - accuracy: 0.4968\n",
            "Epoch 68/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0651 - accuracy: 0.4965\n",
            "Epoch 69/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0627 - accuracy: 0.4974\n",
            "Epoch 70/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0620 - accuracy: 0.4977\n",
            "Epoch 71/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0593 - accuracy: 0.4984\n",
            "Epoch 72/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0595 - accuracy: 0.4983\n",
            "Epoch 73/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0596 - accuracy: 0.4983\n",
            "Epoch 74/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0560 - accuracy: 0.4997\n",
            "Epoch 75/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0546 - accuracy: 0.4998\n",
            "Epoch 76/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0563 - accuracy: 0.4994\n",
            "Epoch 77/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0546 - accuracy: 0.4998\n",
            "Epoch 78/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0524 - accuracy: 0.5002\n",
            "Epoch 79/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0504 - accuracy: 0.5011\n",
            "Epoch 80/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0506 - accuracy: 0.5007\n",
            "Epoch 81/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0503 - accuracy: 0.5008\n",
            "Epoch 82/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0488 - accuracy: 0.5012\n",
            "Epoch 83/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0474 - accuracy: 0.5019\n",
            "Epoch 84/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0473 - accuracy: 0.5018\n",
            "Epoch 85/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0460 - accuracy: 0.5026\n",
            "Epoch 86/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0459 - accuracy: 0.5023\n",
            "Epoch 87/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0462 - accuracy: 0.5022\n",
            "Epoch 88/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0445 - accuracy: 0.5022\n",
            "Epoch 89/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0428 - accuracy: 0.5030\n",
            "Epoch 90/100\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.0423 - accuracy: 0.5030\n",
            "Epoch 91/100\n",
            "183/183 [==============================] - 6s 35ms/step - loss: 0.0417 - accuracy: 0.5032\n",
            "Epoch 92/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0405 - accuracy: 0.5037\n",
            "Epoch 93/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0406 - accuracy: 0.5037\n",
            "Epoch 94/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0399 - accuracy: 0.5038\n",
            "Epoch 95/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0401 - accuracy: 0.5037\n",
            "Epoch 96/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0402 - accuracy: 0.5037\n",
            "Epoch 97/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0390 - accuracy: 0.5041\n",
            "Epoch 98/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0376 - accuracy: 0.5046\n",
            "Epoch 99/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0366 - accuracy: 0.5050\n",
            "Epoch 100/100\n",
            "183/183 [==============================] - 6s 34ms/step - loss: 0.0358 - accuracy: 0.5049\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f54d15a0e80>"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EPOCHS = 100\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "364d0046",
      "metadata": {
        "id": "364d0046"
      },
      "source": [
        "## STEP5. 모델 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db98e621",
      "metadata": {
        "id": "db98e621",
        "outputId": "a0baf445-92e7-4ce6-8ec8-bcb89e594cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def decoder_inference(sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
        "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
        "    sentence = tf.expand_dims(\n",
        "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
        "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
        "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 인퍼런스 단계\n",
        "    for i in range(MAX_LENGTH):\n",
        "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
        "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "        predictions = predictions[:, -1:, :]\n",
        "\n",
        "    # 현재 예측한 단어의 정수\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
        "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "            break\n",
        "\n",
        "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
        "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
        "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output_sequence, axis=0)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e7f6c1",
      "metadata": {
        "id": "76e7f6c1",
        "outputId": "a1bebbb9-c1cf-4be7-cd5a-80ec8e37de0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def sentence_generation(sentence):\n",
        "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
        "    prediction = decoder_inference(sentence)\n",
        "\n",
        "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
        "    predicted_sentence = tokenizer.decode(\n",
        "        [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "    print('입력 : {}'.format(sentence))\n",
        "    print('출력 : {}'.format(predicted_sentence))\n",
        "    print(\"-------------------\")\n",
        "\n",
        "    return predicted_sentence\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072dbb00",
      "metadata": {
        "id": "072dbb00"
      },
      "source": [
        "### 한글만 정규 표현식에 넣어서 진행 했을 때 결과\n",
        "pattern=\"가-힣.?!,\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9937d8",
      "metadata": {
        "id": "af9937d8",
        "outputId": "dd6927a2-6395-4bda-e913-aa5d2b365787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 한가지 소원이 있다면?\n",
            "출력 : 가운데서 만나거나 돌아가면서 희생을 하죠 .\n",
            "-------------------\n",
            "입력 : 심심해?\n",
            "출력 : 노래 불러 드릴까요 ? 북치기박치기박치기 헥헥\n",
            "-------------------\n",
            "입력 : 점심메뉴 선택해줘\n",
            "출력 : 저도 제 마음대로 되면 더 바랄 게 없겠네요 .\n",
            "-------------------\n",
            "입력 : 여행 떠나고 싶어\n",
            "출력 : 큰 꿈여행을 떠나 꿈여행을 하고 아무렇지도 않게 같이 꿈보세요 .\n",
            "-------------------\n",
            "입력 : TV엔 어떤 프로그램이 나오니?\n",
            "출력 : 항상 확실한 의사표시를 했는데도 그렇다는 건 이기적인 것 같아요 .\n",
            "-------------------\n",
            "입력 : 1년뒤에 어떻게 바뀔까요?\n",
            "출력 : 평소에 볼 수 없었던 특별한 모습으로 색다른 데이트를 제안해보세요 .\n",
            "-------------------\n",
            "입력 : 12월에 어디를 가면 좋을까?\n",
            "출력 : 알아가는 단계니 디테일한 건 말 안해도 돼요 .\n",
            "-------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'알아가는 단계니 디테일한 건 말 안해도 돼요 .'"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_generation('한가지 소원이 있다면?')\n",
        "sentence_generation('심심해?')\n",
        "sentence_generation(\"점심메뉴 선택해줘\")\n",
        "sentence_generation('여행 떠나고 싶어')\n",
        "sentence_generation('TV엔 어떤 프로그램이 나오니?')\n",
        "sentence_generation('1년뒤에 어떻게 바뀔까요?')\n",
        "sentence_generation('12월에 어디를 가면 좋을까?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10ab91f",
      "metadata": {
        "id": "f10ab91f"
      },
      "source": [
        "### 한글과 영어를 정규 표현식에 넣어서 진행 했을 때 결과\n",
        "pattern1 = \"가-힣a-zA-Z.?!,\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a269c8",
      "metadata": {
        "id": "b0a269c8",
        "outputId": "2cf92543-2bca-44fb-8777-ef2a6967811f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 한가지 소원이 있다면?\n",
            "출력 : 요즘 상영하는 영화나 맛집부터 시작해보는 것도 좋을 듯합니다 .\n",
            "-------------------\n",
            "입력 : 심심해?\n",
            "출력 : 친구들과 연락해보세요 . 당장 그럴 거예요 . 무엇이 그럴 수 있어요 . 무엇이 있어요 .\n",
            "-------------------\n",
            "입력 : 점심메뉴 선택해줘\n",
            "출력 : 돌아보면 다그치지 않고 화내지 않아도 될 일이었을 거예요 .\n",
            "-------------------\n",
            "입력 : 여행 떠나고 싶어\n",
            "출력 : 좋은 생각이에요 . 좋은 생각이에요 . 혼자하는 여행은 기분전환에 좋은 생각이에요 .\n",
            "-------------------\n",
            "입력 : TV엔 어떤 프로그램이 나오니?\n",
            "출력 : 수학 공식처럼 정의 내릴 수 있다면 그런 거예요 .\n",
            "-------------------\n",
            "입력 : sd카드 사고 싶어\n",
            "출력 : 거울을 보면서 얼굴 망가뜨리기 놀이 해 보신 적 있으세요 ? 의외로 재미 있어요 .\n",
            "-------------------\n",
            "입력 : 1년뒤에 어떻게 바뀔까요?\n",
            "출력 : 일부로 부추기면 가끔 안 좋은 결과가 나오기도 해요 .\n",
            "-------------------\n",
            "입력 : 12월에 어디를 가면 좋을까?\n",
            "출력 : 보통 남자들은 아무 여자에게나 하트를 붙여서 말하지 않아요 .\n",
            "-------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'보통 남자들은 아무 여자에게나 하트를 붙여서 말하지 않아요 .'"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_generation('한가지 소원이 있다면?')\n",
        "sentence_generation('심심해?')\n",
        "sentence_generation(\"점심메뉴 선택해줘\")\n",
        "sentence_generation('여행 떠나고 싶어')\n",
        "sentence_generation('TV엔 어떤 프로그램이 나오니?')\n",
        "sentence_generation('sd카드 사고 싶어')\n",
        "sentence_generation('1년뒤에 어떻게 바뀔까요?')\n",
        "sentence_generation('12월에 어디를 가면 좋을까?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84980442",
      "metadata": {
        "id": "84980442"
      },
      "source": [
        "### 숫자, 한글을 정규 표현식에 넣어서 진행 했을 때 결과\n",
        "pattern2 = \"0-9가-힣.?!,\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386e4833",
      "metadata": {
        "id": "386e4833",
        "outputId": "a5f47576-daeb-4719-a02d-9fb3bbff03e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 : 한가지 소원이 있다면?\n",
            "출력 : 영화관 공원 놀이동산 바다 산 축제 도서관 엄청 많죠 .\n",
            "-------------------\n",
            "입력 : 심심해?\n",
            "출력 : 노래 불러 드릴까요 ? 북치기박치기박치기 헥헥\n",
            "-------------------\n",
            "입력 : 점심메뉴 선택해줘\n",
            "출력 : 화학 물질이 접촉하는 것처럼 반응이 일어나서 완전히 바뀌게 되죠 .\n",
            "-------------------\n",
            "입력 : 여행 떠나고 싶어\n",
            "출력 : 저도 만날 사람 만날 때 같이 가는 건 같이 가는 사람은 안됩니다 .\n",
            "-------------------\n",
            "입력 : TV엔 어떤 프로그램이 나오니?\n",
            "출력 : 마음 먹은 것만으로도 절반을 해낸 거라 생각해요 .\n",
            "-------------------\n",
            "입력 : sd카드 사고 싶어\n",
            "출력 : 목록에서 없애는 건 어떨까요 . 눈에서 멀어지면 미련도 덜해집니다 .\n",
            "-------------------\n",
            "입력 : 1년뒤에 어떻게 바뀔까요?\n",
            "출력 : 마음은 마음대로 되지 않아요 . 이 고민지다고 생각하는 사람은 좋겠네요 .\n",
            "-------------------\n",
            "입력 : 12월에 어디 가면 좋으니?\n",
            "출력 : 사랑한다면 자존심을 굽힐 줄도 알아야 겠죠 .\n",
            "-------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'사랑한다면 자존심을 굽힐 줄도 알아야 겠죠 .'"
            ]
          },
          "execution_count": 236,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_generation('한가지 소원이 있다면?')\n",
        "sentence_generation('심심해?')\n",
        "sentence_generation(\"점심메뉴 선택해줘\")\n",
        "sentence_generation('여행 떠나고 싶어')\n",
        "sentence_generation('TV엔 어떤 프로그램이 나오니?')\n",
        "sentence_generation('sd카드 사고 싶어')\n",
        "sentence_generation('1년뒤에 어떻게 바뀔까요?')\n",
        "sentence_generation('12월에 어디 가면 좋으니?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b4e5955",
      "metadata": {
        "id": "0b4e5955"
      },
      "source": [
        "## 회고\n",
        "\n",
        "- 한국어로 챗봇 만들기 프로젝트를 진행하면서 트랜스포머의 모델에 대한 이해도가 높지 않아서 중점을 데이터 전처리에 두고 프로젝트를 진행했다. 프로젝트를 진행하면서 epoch의 변화와, 단어 토크나이징에 변화를 주면서 모델을 학습시켜봤다.  \n",
        "- 먼저 epoch에 대해선 10, 50, 100, 200으로 학습을 진행하였는데, 10, 50 보다는 100일 때, 문장 답변의 퀄리티가 좋았고, 200일 때는 100과 비교했을 때, 좋았던 경우도 있고, 부정확한 경우도 있었다. 그래서 200일 때는 오버피팅이 발생했다고 생각했고, epoch는 100으로 설정해서 학습을 진행했다.  \n",
        "- 다음으로 단어 토크나이징은 3가지로 나누어서 진행했는데, 결론은 영어와, 숫자는 데이터가 너무 적어서 결과가 좋지 못했다. 영어는 부정확한 답변을 알려주었고, 숫자는 그나마 영어 보다는 좋았지만, 그래도 부정확한 답변을 알려주었다. 그래서 이 모델을 학습시킨다면, 질문을 한글위주로 하는것이 좋을것 같다. 아니면 데이터를 추가해서 학습시키는것이 좋을것 같다.  \n",
        "- 마지막으로 데이터셋에 대해 살펴 보았는데, 문장 구조가 비슷한 경우가 많았고, 데이터도 많지 않다보니, 학습시킨 내용과 비슷한 질문을 했을 땐 답변을 잘 해주었지만, 어려운? 질문을 했을 땐 답변을 잘 하지 못하는 결과를 보여주었다. 학습과 관련해서 정확도가 높다고 무조건 좋은 답변을 해주는 것은 아니었다(정확도가 0.3과 0.5는 질문의 답변 퀄리티가 차이가 낫지만, 0.5와 0.6의 정확도에 대한 질문의 퀄리티 차이는 거의 없었다.).\n",
        "- 성능을 높이고자, 하이퍼 파라미터를 변경해 보았지만, 사용하는 데이터 셋의 한계가 있기도 했고, 최적의 하이퍼파라미터를 구하기(ex gridsearch)엔 시간이 부족하여 실행해보지 못했다. 추후에 트랜스포머에 대해 좀더 배워보고 데이터 셋에 알맞는 학습을 진행해보겠다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb62e15",
      "metadata": {
        "id": "8fb62e15"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4241752",
      "metadata": {
        "id": "c4241752"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8aa0ff6",
      "metadata": {
        "id": "c8aa0ff6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05efb5a3",
      "metadata": {
        "id": "05efb5a3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}